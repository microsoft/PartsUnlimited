##Scaling Redis##

Redis is an open source, in-memory data structure store, used as database, cache and message broker. It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs and geospatial indexes with radius queries. 

Azure Redis Cache is a hosted instance of Redis. It gives you access to a secure, dedicated Redis cache, managed by Microsoft and accessible from any application within Azure. 

In this section, we’ll review common pitfalls with connection management and identify approaches to keep the items in your cache up to date. We'll also look at what happens when the cache is failing over to a secondary - or is offline - and how we can detect these failure conditions to ensure the application is resilient during this period.  These suggestions will also address how to improve performance 

###Cache options###

Because of the versitile nature of Redis there are several applications of how Redis can be integrated into your application. This includes some options that link into existing ASP.NET extensibility points, for example: 

* Output cache - [Azure Redis ASP.Net Output cache provider](https://azure.microsoft.com/en-us/documentation/articles/cache-asp.net-output-cache-provider/)
* Session state - [Azure Redis Session State provider](https://azure.microsoft.com/en-us/documentation/articles/cache-asp.net-session-state-provider/)

The majority of this example will look at building out Redis within our business logic layer to store frequently accessed application data.

###Setup###

Redis in Azure comes in three service tiers Basic, Standard and Premium. 
Basic is ideal for development and testing. For use in a production environment you would need to monitor cache throughput to ensure you are not reaching the limits.
For replication and failover support you will need to use a Standard cache. To gain disk persistence, cluster and scale out of your cached items you will need to further upgrade to Premium.

For more information regarding the tiers, please refer to [Redis Tiers](https://azure.microsoft.com/en-us/pricing/details/cache/). 

For on setting up Redis cache in Azure, see [How to Use Azure Redis Cache](https://azure.microsoft.com/en-us/documentation/articles/cache-dotnet-how-to-use-azure-redis-cache/)

###Client Libraries###

There are multiple client libraries across several languages. Each library can be used to connect into Redis servers. Because Parts Unlimited is an ASP.NET application, this example will use a .NET library. The two most common and recommended client libraries are ServiceStack.Redis and StackExchange.Redis.  For the purposes of this document, we have chosen to use StackExchange.Redis.
> StackExchange.Redis is used by Microsoft projects like the above mentioned Azure Redis ASP.Net Output cache provider and Azure Redis Session State provider.


See [here](http://redis.io/clients) for an up to date list of client libraries which support Redis.

StackExchange.Redis leaves it up to us as how we want to serialize and deserialize the items we want to cache. Because of this after determining a client library, we must also determine which serialization library to use. A couple of contenders are [Json.Net](http://www.newtonsoft.com/json) and [Protobuf](https://code.google.com/p/protobuf-net/). In this example we have opted to use Json.Net for its ease of use and also to allow us to look at the cached items in a human readable format using the [Redis console](https://azure.microsoft.com/en-us/documentation/articles/cache-configure/#redis-console).

###Connection###

Once the Redis cache has been configured within Azure, the first task is to setup a connection. The StackExchange.Redis client uses multiplexing through a single connection. Multiplexing is a way of sending multiple signals or streams of information over a communications link at the same time. By providing this implementation, we as users of a multiplexed connection can utilize the same connection concurrently across multiple threads quite safely.

The recommended usage is to create a single instance of the client and use this instance for all further commands issued to the cache. See the Parts Unlimited implementation of the connection [here](../../src/PartsUnlimitedWebsite/Cache/PartsUnlimitedRedisCache.cs) 

    private static Lazy<ConnectionMultiplexer> lazyConnection = new Lazy<ConnectionMultiplexer>(() =>
    {
        return ConnectionMultiplexer.Connect(configurationOptions);
    });

    public static ConnectionMultiplexer Connection
    {
        get { return lazyConnection.Value; } 
    }

> Because StackExchange.Redis uses multiplexing we should re-use the same instance of the ConnectionMultiplexer across the application. If you are wrapping up the Redis logic in one class ensure that this class is a singleton. Modern dependency injection frameworks like Ninject and Unity have a way to achieve this at mapping time. To validate that multiple caches connections are not being created you can check the Azure portal `Connections` count metric. For example if you had your application deployed to six instances of an Azure website, you would expect at one time to only have six live connections open. To look at your Redis metrics follow the instructions outlined [here](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-monitor/).

ConnectionMultiplexer implements `IDisposable` and is disposed when no longer required. We are explicitly not making use of a `using` statement. It's uncommon that you would use a `ConnectionMultiplexer` with a small life time -the idea is that it's reused. 
This is achieved in two ways

1. Using a Lazy<ConnectionMultiplexer> internal to the `PartsUnlimitedredisCache` class, this ensures that only ever a single instance of the `ConnectionMultiplexer` can be created within this instance of a `PartsUnlimitedredisCache`. 
2. Only ever creating a single instance of the `PartsUnlimitedRedisCache` class within the application. 

> This is configured using the out of the box vNext dependency injection (DI) framework. 
For each class you can define the life cycle of the instance which will be created. The options are Instance, Transient, Singleton, Scoped. In our case for the `PartsUnlimitedRedisCache` we will use Singleton. This means that a single instance is created and it acts like a singleton. For more information on the VNext DI  framework see : [Dependency Injection in ASP.NET vNext](http://blogs.msdn.com/b/webdev/archive/2014/06/17/dependency-injection-in-asp-net-vnext.aspx)

###Query###

Querying data out of Redis is simple using the StackExchange.Redis libraries.
Below we retrieve a string based on a `key` and deserialize using the Json.net libraries into a `cacheitem`.  

    RedisValue redisValue = Connection.Database.StringGet(key);
    T cacheItem = JsonConvert.DeserializeObject<T>(redisValue);

Redis is not just a key-value pair (KVP) storage mechanism, it also supports other more complex data structures like sorted and ordered sets and lists and also supports transactions and mass insert. For more information see [An introduction to Redis data types and abstractions](http://redis.io/topics/data-types-intro), [Transactions](http://redis.io/topics/transactions) and [Mass insert](http://redis.io/topics/mass-insert)

###Store###

Storing data into Redis is equally simple using the StackExchange.Redis libraries.
Below we serialize an object using the Json.net libraries and store with a corresponding `key`.
The `span` attribute lets Redis know the expiration time / time to live (TTL) of the cache item.

	string stringValue= JsonConvert.SerializeObject(cacheItem, Formatting.None, settings);
    Connection.Database.StringSetAsync(key, stringValue, span, When.Always, commandFlags);

>During development it maybe helpful to turn the JSON.Net serialization formatting to from `Formatting.None` to `Formatting.Indent` to assist with human readability. Prior to releasing you should switch this to reduce memory and network usage.

There are other controls when calling into `StringSetAsync` which define the TTL, replace and overwrite behaviors, and fire and forget. For further explanation on the workings of the StackExchange.Redis libraries. See [Basic Usage](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Basics.md).

Absolute expiration time is a native feature of Redis. To implement a sliding expiration we need to manage this ourselves. This can be achieved by resetting the TTL after each command. We have implemented this at query time by storing the SlidingCacheTime in the cache alongside the cached item. When the item is subsequently loaded we reset the TTL based on the original.

	CacheItem<T> cacheValue = JsonConvert.DeserializeObject<CacheItem<T>>(redisValue);
    if (cacheValue.SlidingCacheTime.HasValue)
    {
    	var timeSpan = cacheValue.SlidingCacheTime.Value;
	    await Database.KeyExpireAsync(key, timeSpan, CommandFlags.FireAndForget);
    } 

>When we reset the expiry of the database key we are passing in `CommandFlags.FireAndForget`. This will ensure that we don't block responding to the user while we do another network call to update the key with new expiry time.

When pushing items into our cache we need to decide on the TTL and whether this is absolute or sliding.

1. Absolute or Sliding

Depending on the type of data will determine on whether we use absolute or sliding, as a starting point use absolute unless you have a justification for using sliding. 
One example of some information which you may store in the cache using a sliding TTL is user session information which you want to expire after a period of in-activity. Otherwise, most other type of cache data you would want predictable behavior around how often the data will be refreshed from the source data store.

2. Time to live

Guidance around setting of the expiration time depends on your application. It’s a balance between being too short vs too long, even if your data is rapidly changing, caching your data for a second changes your database access from unbounded (infinite) to 60 times per minute (finite). Given user round trip to web server and rendering time it is unlikely that 1 second will introduce consistency errors, where as a larger value might.

Finally another consideration is the cost of the data retrieval. If you are calling a service which is expensive (time or cost) or the service is rate limited, using Redis as an intermediary to buffer your calls could be an elegant solution.

###Parts Unlimited Cache###

All commands which make use of caching in our business layer adhere to an interface. See [IPartsUnlimitedCache.cs](../../src/PartsUnlimitedWebsite/Cache/IPartsUnlimitedCache.cs). This interface defines common behavior and is not specific to any particular cache implementation. 

        Task SetValueAsync<T>(string key, T value, PartsUnlimitedCacheOptions options);
        Task<CacheResult<T>> GetValueAsync<T>(string key);
        Task RemoveAsync(string key);

By creating this contract and not directly using the underlying cache constructs in our code we are able to switch out the implementations of our cache with other implementations at run time. See `SetupCache` method in the [Startup.cs](../../src/PartsUnlimitedWebsite/Startup.cs) class where we switch between an in memory cache and Redis cache based on the existence of Redis configuration.
	
> With all external dependencies it is a good practice to wrap the behavior behind an interface. This adheres to one of the Gang of Four design techniques - program to an 'interface', not an 'implementation'. For further information see : [Design Principles from Design Patterns](http://www.artima.com/lejava/articles/designprinciplesP.html)
 
For Parts Unlimited we have implemented the [Cache-Aside Pattern](https://msdn.microsoft.com/library/dn589799.aspx). This pattern loads data on demand into the cache from the underlying data store. Below is a snippet of this based from [CacheCoordinator.cs](../../src/PartsUnlimitedWebsite/Cache/CacheCoordinator.cs).

	//Try and load from the cache
	var result = await _cache.GetValue<T>(key);
	
	//Return cached item if it exists
	if (result.HasValue)
	{
		return result.Value;
	}

	//Load item from the underlying source and populate the cache.
	var sourceValue = await sourceLoader.Value;
	await _cache.SetValue(key, sourceValue, options.CacheOption);

While Redis is fast, it still requires a network hop and deserialization / serialization. For frequently used data, it may be worth having multiple levels of caching. A multi-level cache stores two copies of the cached data. One copy in an external cache, such as Redis, and another copy in an in-memory cache, such as MemCache. The flow has a short expiration of the first level in-memory cache and allows it to fall back to the second level Redis cache to restore data.

An in-memory cache is private, so each instance of the application could potentially maintain inconsistent versions of the cached data. This becomes relevant when your site is backed by multiple Azure instances. To address this, typically the first level in-memory cache TTL would be less than the Redis cache TTL.

We have implemented multi-level cache in Parts Unlimited by simply creating another implementation of `IPartsUnlimitedCache.cs`. This implementation internally delegates the workload out to the first and the second level caches.

	public class PartUnlimitedMultilevelCache : IPartsUnlimitedCache
    {
       	....
        public async Task SetValue<T>(string key, T value, PartsUnlimitedCacheOptions options)
        {
            if (options.ShouldApplyToMultiLevelCache)
            {
                await _memoryCache.SetValue(key, value, options.SecondLevelCacheOptions);
            }

            await _redisCache.SetValue(key, value, options);
        }

        public async Task<CacheResult<T>> GetValue<T>(string key)
        {
            var memoryResult = await _memoryCache.GetValue<T>(key);
            if (memoryResult.HasValue)
            {
                return memoryResult;
            }

            return await _redisCache.GetValue<T>(key);
        }  
		....
	}

###Retry / Failover###

As cloud developers we need to consider the distributed nature of the application which we are building. With this distribution comes risks of communication failures or shared infrastructure being over loaded. 
Because these transient errors are temporary in nature, we need to consider patterns to reduce the impact on our users. 

By using Azure and one of the standard or premium Azure Redis tiers, you automatically have replication and failover built into the cache. This is important as in Azure nodes can go down for a variety of reasons like servicing, hardware failures, patching etc. While Azure is failing over to the secondary you potentially may see some failed requests.

####Minimizing impact####

The StackExchange.Redis client has connection management retry logic built in to the libraries. This retry logic is only supported while establishing the initial connection to the cache and does not apply to operations and commands against the cache. The retry logic also does not have a configurable delay between retry attempts -simply it retries connecting after the connection timeout expires for the specified number of retries.

To have more control of our retry logic, and also be able to apply this logic to more operations than just the initial connection you could use a transient error handling framework. Below are two available frameworks :

- [Transient fault handling application block](https://msdn.microsoft.com/en-us/library/dn440719.aspx)
- [Polly](http://www.hanselman.com/blog/NuGetPackageOfTheWeekPollyWannaFluentlyExpressTransientExceptionHandlingPoliciesInNET.aspx)

There is no direct support for Redis Cache in the either of the two above framework. To understand whether a Redis error is transient or not, we would need to customize the behavior of the framework.

To achieve this, you could interrogate the internals of the `RedisConnectionException` to determine if it's a connection error which should be retried based on the [ConnectionFailureType.FailureType](https://github.com/StackExchange/StackExchange.Redis/blob/master/StackExchange.Redis/StackExchange/Redis/ConnectionFailureType.cs).  Otherwise, if the problem is a timeout, we should also retry.

	private static readonly List<ConnectionFailureType> _transientTypes = new List<ConnectionFailureType>
    {
        ConnectionFailureType.UnableToConnect,
        ConnectionFailureType.Loading,
        ConnectionFailureType.ConnectionDisposed,
        ConnectionFailureType.UnableToConnect
    };

    public bool IsTransient(Exception ex)
    {
        var connectionException = ex as RedisConnectionException;
        if (connectionException != null)
        {
            return _transientTypes.Contains(connectionException.FailureType);
        }

        if (ex is TimeoutException)
        {
            return true;
        }

        return false;
    }

####Failure alternatives####

In the case where we sustain repeated transient errors or non-transient errors, we need to ensure that our application is resilient when we cannot reach the cache. Failing all requests back to the underlying data source would potentially place undue load on the data provider and cause a denial of service.

> If you relied on reading from the cache as part of the checkout process in an e-commerce store and this failed, this may be a candidate to read from the underlying data store. Comparing this to a failure to read from the cache for display purposes, this is less severe and perhaps acceptable to direct the user to an error page.

Expanding on our previous code snippet from [CacheCoordinator.cs](../../src/PartsUnlimitedWebsite/Cache/CacheCoordinator.cs) we can fallback to the source system when the cache key is not found. We are using the `options.CallFailOverOnError` to distinguish whether the item being requested should fall back to the source system or fail.

	Lazy<Task<T>> sourceLoader = new Lazy<Task<T>>(loadFromSource.Invoke);
	try
    {
        var result = await _cache.GetValueAsync<T>(key);
        if (result.HasValue)
        {
            return result.Value;
        }

        //initial population.
        var sourceValue = await sourceLoader.Value;
        await _cache.SetValueAsync(key, sourceValue, options.CacheOption);

        if (sourceValue == null && options.RemoveIfNull)
        {
            await Remove(key);
        }

        return sourceValue;
    }
    catch (Exception ex)
    {
        _telemetryProvider.TrackException(ex);
    }

    //Cache has failed, fail back to source system.
    if (options.CallFailOverOnError || sourceLoader.IsValueCreated)
    {
        return await sourceLoader.Value;
    }

	throw new InvalidOperationException($"Item in cache with key '{key}' not found");

####Invalidate####

When items are put into a cache we generally specify a TTL (time to live) and whether this is a sliding or absolute period.

E.g. Within Parts Unlimited the product category has a absolute TTL of 10 minutes. 
After 10 minutes, Redis will evict this product category on our behalf. The next user who makes the same request will cause a cache miss. Based on the 'Cache-Aside pattern', this will trigger a load from the source system to load the underlying record, push this into Redis, and then return the product category to the application. If there are 1000 concurrent users requesting the categories at the same time, it is possible that all 1000 users will have a cache miss and trigger a load from the source system potentially causing a denial of service.

One approach to avoid this waiting time and ensure predictable performance is to prime the cache.
Priming the cache is when the application is responsible for pre-populating the cache with items from the source system. Further on from this it can be regularly updated on a schedule replacing expired keys.

> A possible implementation could be a webjob running in the back ground using Azure Scheduler or triggered through Azure storage / Azure queue. See [Create a .NET WebJob](https://azure.microsoft.com/en-us/documentation/articles/websites-dotnet-webjobs-sdk-get-started/)

####Operations / Monitoring####

From within the Azure portal, you can review cache misses vs cache hits, memory and cpu usage and load.
Based on these metrics if you find that your cache is no longer meeting the requirements of your application, you can scale you cache. For more information on monitoring see [How to monitor Azure Redis Cache](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-monitor/) and for more on [How to scale Azure Redis Cache](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-scale/)

You can also introduce your own monitoring and scaling operations across the cache.  Example implementations based on the Azure Management Libraries (MAML) are shown [here](https://github.com/rustd/RedisSamples/tree/master/ManageCacheUsingMAML). To alter the Redis SKU using MAML, see ['How to automate a scaling operation'](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-scale/#how-to-automate-a-scaling-operation). Scaling of Redis Cache is currently in preview and has some limitations.

As your Redis cache is scaled, depending on the source and destination service tier will determine whether data is lost or it's preserved. For more information see [Will I lose data from my cache during scaling](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-scale/#will-i-lose-data-from-my-cache-during-scaling)