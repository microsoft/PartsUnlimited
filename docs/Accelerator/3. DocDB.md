##DocumentDB; Storage and indexing of Arbitrary Data Structures##

Microsoft Azure DocumentDB is the highly-scalable NoSQL document database-as-a-service that offers rich query and transactions over schema-free data, helps deliver reliable and predictable performance, and enables rapid development.

To highlight the capability of the schema free database we will show an architectural approach to address the schema free and embrace loose typings. This will be shown through storage of the product catalogue with products which contain deeply nested attributes and show an approach how they can be shown and queried across.

We also highlight common mistakes made when implementing and querying DocumentDB and ways in which to avoid or address these.

###Setup###
For this section we assume basic knowledge of setting up and consuming DocumentDB inside your application. A comprehensive guide to getting started with DocumentDB can be found [here](https://azure.microsoft.com/en-us/documentation/articles/documentdb-create-account/)

Modifications to Parts Unlimited for this section will require a DocumentDB database account created in Azure and linked to your application.
		
###Managing collections and items###

* Talk about wrap around interface to switch out SQL and DocDb

**TODO - SB - insert example interface snippet here**

####Schema free storage approach####

"let's consider that you are building an e-commerce site where you sell everything from books to video games to laptops. It is really hard to fit this kind of data in to a tabular structure. Creating a column for each attribute doesn't scale because there are too many varying attributes among your various products. How much ram does your book? Who is the author of the laptop? Creating a table for each product type is cumbersome because you have a rather expansive product catalog. Creating and maintain 1000s of tables for 1000s of product types is a maintenance nightmare. Storing everything as JSON in a single varchar column doesn't perform well. You lose the ability to index and query off individual attributes. Storing a heterogeneous data (schemas that vary quite a bit) in a schema-agnostic database is easy; just store the data and query off the fields you need. Simple."

* Talk about Schema free storage, still need to define a schema someplace to use (Whether it’s in the Application or UI layer )
	* Talk about option of storing schema against DB

####Collection partitioning strategy####
	
Collections act as highly available data partitions for document storage and processing. A common practice when migrating from a SQL database implementation to DocumentDB is to define an individual DocumentDB collection for each table or item type. It is however important to consider that a collection within DocumentDB can store heterogeneous items with a diverse range of content and not tied to a particular type or schema. Additionally, Azure DocumentDB pricing is based on a "per collection" model, with larger amounts of collections incurring higher costs. It is therefore recommended to have a collection partitioning strategy that minimises the amount of total collections, and to view collections as units of partitions that provide boundaries for transactions and query execution rather than tables, thus having these collections driven by capacity requirements (individual collection have a 10GB capacity constraint), or by throughput requirements for the individual collection based on the data housed within, as individual collections can be assigned different performance tiers (see the throttling section below).

Different "types" of items can be stored within an individual collection by creating a "type" discriminator property for each entry within the collection to enable filtering by item type. e.g.

	SELECT * FROM c WHERE c.type = "Product"

It is worth noting that by default, all properties within DocumentDB are hash-indexed, resulting in a negligible performance hit for filtering for items in this manner. However if you have opted to specify a custom indexing policy, the "type" property should be configured to be indexed in order to assist in performance when filtering by type. More information on DocumentDB indexing policies can be found [here](https://azure.microsoft.com/en-us/documentation/articles/documentdb-indexing-policies/).

From a performance perspective, an added benefit of structuring collections in a manner that collocates heterogeneous documents in a singular collection is that this provides the most effecient read pattern when querying across a set of items, as the initial setup work that includes fetching the physical address of a partiion, as well as "warming" the connection pool only has to occur once, providing reduced request latency for subsequent calls to the individual collection.

###Querying DocumentDB###
	
####Complex arbitrary JSON documents####

* Allow users to see a BoM (bill of materials) type structure and to search for similar items deep within that structure where search relies on indexed arbitrary structure
	* In particular we need to show the power of being able to perform a fast, strongly typed query, on arbitrary data structures
	* Call out the indexing options
	* Something where it matters to be numeric, and it matters to be a string, To show that it's not just Full Text Indexing, and we want to be deeply nested

* Search for related products with SQL like query

####Caching query objects####

A common mistake is to create a new DocumentClient each time the application intends to perform a request to DocumentDB. Each DocumentClient instance is thread-safe and performs efficient connection management and address caching when operating in Direct Mode. Therefore, creating a new instance of DocumentClient is an expensive operation that can have performance implications. To allow efficient connection management and better performance by DocumentClient, it is recommended to use a single instance of DocumentClient per AppDomain for the lifetime of the application.

In Azure DocumentDB, each document has a system-generated SelfLink. These SelfLinks are guaranteed to be unique and immutable for the lifetime of the document, and reading a single document using a SelfLink is commonly referenced as the most efficient way to consume a single document. It is important to note however, that a common mistake is to create new instances of 'GetOrCreateDatabase' or 'GetOrCreateCollection' every time a reference to a database or collection is needed in order to retrieve SelfLinks. This results in multiple queries to DocumentDB for every single intended operation, and can result in exceeding your request quota and getting throttled. It is therefore recommended to cache these objects whenever suitable if they are required by the application.

It is important to note that today, we can do away with SelfLinks to a large degree, avoiding this issue entirely. An UriFactory can be used to construct links that are based on the ID property of items, and therefore a query for databases or collections are not needed in many cases.

**TODO - SB - Insert sample of URI builder with ID bases routing **
		
####Managing throttling####

Azure DocumentDB implements a reserved throughput model for each individual collection that resides in your database account. Throughput requirements can be managed for individual collections as requirements from the application change by setting their respective performance levels, which can be viewed [here](https://azure.microsoft.com/en-us/documentation/articles/documentdb-performance-levels/)

As a result of throughput throttling on individual collections, it is possible to observe confusing behaviour inside your application as certain requests to DocumentDB succeed and others fail once they have exceeded their individual request quota and get throttled. It is therefore important to account for this behaviour within your application.

When the application exceeds the allowed throughput for the collection, DocumentDB will pre-emptively ends the request and respond with HTTP status code 429 (Request too large), and return a "x-ms-retry-after-ms header representing the amount of time (in milliseconds) that the application must wait before attempting another request:

	HTTP Status 429,
	Status Line: RequestRateTooLarge
	x-ms-retry-after-ms :100
	
When using the DocumentDB .Net SDK with LINQ, the SDK will automatically retry the failed operation internally when it encounters an HTTP 429. There are however scenarios where default throttling exception behaviour from the SDK may not be sufficient, and in such cases the application can be modified to handle the RequestRateTooLargeException. An example of dealing with the exception can be found below:

	private static async Task<V> ExecuteTaskWithThrottlingSafety<V>(DocumentClient client, Func<Task<V>> func)
	{
		TimeSpan delayTime = TimeSpan.Zero;

		while (true)
		{
			try
			{
				return await func();
			}
			catch (AggregateException ae) when (ae.InnerException is DocumentClientException)
			{
				DocumentClientException de = (DocumentClientException)ae.InnerException;
				if((int)de.StatusCode == 429)
				{
					delayTime = de.RetryAfter;
				}
				else
				{	
					throw;
				}                   
			}
			
			await Task.Delay(delayTime);
		}
	}	

**TODO - SB - reference fault handling stuff (same as Redis ??)**

####Comparison of typed queries vs JSON to client####

**TODO - SB - chat about being aware of overhead and how to avoid it**

* Compare Typed query VS JSON document to client
	* JSON direct to client exists currently. 
	* When a document is retrieved, the internal deserialization process is not triggered untill a property of the document is accessed. e.g. var id = Document.Id.  
	* However, if  a JsonWriter is used on the document object to get the string, the deserialization overhead is skipped.
	* Additionally, you load from string directly instead of again deserializing string to object and then us doing object to string process again. Resource.LoadFrom does this currently.

###Parts Unlimited updates###

* Re work product storage to utilize DOC DB
* Store something other than products ?? - To show multiple collections VS table
* Search for related products with SQL like query
	* Include Manufacturer, model, **year number**, country origin. Search for products with same and year +/- 3 years as they are similar.
	* Display all products – top 10 – Ryan call out.
